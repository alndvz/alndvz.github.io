---
layout: post
title:  "Thoughts on Caching"
date:   2016-06-25 12:30:00 +0200
categories: notes
---

How we can cache search results?

Some strategies that come to mind are:
1. Execute the top N number of searches continually caching the results each time. Searches that have not been cached need to be satisfied on demand.
- Move the search mechanism itself closer to the user. The number of entities that we want to be searchable is not as large as to prohibit it's duplication. Move the mechanism that does the search, along with it's data.

## Point 2

Lets explore point 2. What would moving the search mechanism look like? Before we can answer that question we need to define the search mechanism itself.

A user begins a search by specifying a set of parameters. These are inputted through some type of user interface. From the interface the parameters get passed to search server, which in turn returns search results back to the user interface.

The responsiveness of the search through the interface is the key metric that we are trying to improve through this exercise. If the interface and the search results are delivered through the same mechanism, their performance characteristics are bound to each other. 

The user interface can be delivered through a CDN due to the fact that the staleness of it's cache is not of concern. The search results for a set of parameters cannot be returned via a CDN because staleness is a problem if we want our results to be as real time as possible.

Real time results are not the only problem when using a CDN to cache the search results, or any type of dumb cache like it. More obscure parameter and result sets will not benefit from caching due to their infrequency of use, their cache will most likely expire before being called.

Enter the search mechanism, it's requirements are that it take a set of parameters, and return search results. The search results can be returned as multiple representations, some being machine friendly, some being human friendly.

In other words, the search mechanism is a micro service that can take a set of parameters and return JSON, or HTML. The HTML could be loaded into the center of a user interface using a simple AJAX call. The user interface will then be responsible for styling.

To put it clearly, the search mechanism is defined as a service that gets sent a set of parameters and returns a representation of the results based on an internal store of entities.

While the searching itself is not a simple task, this has been solved in the context of this experiment. By making use of an already built search technology and then building an API on top of that we can reason about the mechanism simply and in isolation.

Entities within the search mechanisms store, due to the fact that our goal is to search a relatively low number of entities, can be viewed as ephemeral and allow us to treat the search mechanism as something that can be thrown away at will and replaced, given a short time, the entities within the search mechanism will be added again.

From here on the search mechanism will be referred to as the search service. 

## Peering into the Search Service

What components make up the Search Service? 
- Search server, like Apache SOLR
- API acting as a wrapper to the search server
- Web Server, serving the API
- Index updating process

Each instance of the search service will have it's own instance of the search server. 

### Indexing the Search Server

**Experiment**: Add a message for each entity in the main datastore that should be present in the search index, to a queue. Anytime any part of the data that makes up the entity in the source database changes put a message on the queue. Include the all of the data for the entity in the message.

Consume messages for these queues using the index updating process. The entity in the index will be replaced by the entity in the message from the queue.

A different set of entities could exist in each index within each Search Server. What kind of problems could this cause? The same search done twice, in short succession, could return a different set of entities. When going back and forward in a set of pages of entities, the contents of the pages will change.

Are there ways to overcome this issue by leveraging browser technologies?

The client (the browser) will need to implement **Monotonic Read Consistency**. When the client has read the value of an entity, an older value will never be displayed.

How can this be implemented though? How can this model solve the problem of search results changing unexpectedly?

Surely the set of entities for a search result will need to be serialized and stored in a place the client can access easily and efficiently. This does seem like it would involve too much overhead for mobile clients. 

Moving this kind of storage to the API layer could solve the problem. But it would create other problems and may just be moving the same problems to a different place.

Inconsistency within the search results may not even be a problem. There is probably a low chance that this will actually cause a problem. In fact no matter what type of consistency is implemented, the problem will always exist. Entities will always be removed. 

Under low load conditions an update to an entity will arrive at all instances of the search server within a very short amount of time.

Letting the browser handler caching of search results may be a simple solution, cache a page for 30 seconds, or a minute, that way if the user returns to that page quickly, it will not have changed. Problem is the caching is not a guarantee in the browser. 
